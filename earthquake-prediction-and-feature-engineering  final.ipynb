{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-04T11:18:50.649240Z","iopub.execute_input":"2022-03-04T11:18:50.649570Z","iopub.status.idle":"2022-03-04T11:18:53.462978Z","shell.execute_reply.started":"2022-03-04T11:18:50.649483Z","shell.execute_reply":"2022-03-04T11:18:53.462341Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Importing libraries required","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt                  #Data Visualization\nfrom tqdm import tqdm_notebook\nimport seaborn as sns\nimport pickle\nimport joblib\nimport pywt           #PyWavelets is a free Open Source library for wavelet transforms in Python\nimport warnings\nwarnings.filterwarnings(\"ignore\")                  # Donot display warning messages.\nfrom sklearn.preprocessing import StandardScaler   #Data Scaling \nfrom sklearn.model_selection import GridSearchCV   #Hyperparameter optimization\nfrom sklearn.svm import NuSVR, SVR                 #Support vector Machine Model\nfrom sklearn.kernel_ridge import KernelRidge       #kernel ridge model\nfrom catboost import CatBoostRegressor, Pool       #Catboost Model\nimport xgboost as xgb                              #XgBoost Model\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom prettytable import PrettyTable\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:19:24.153468Z","iopub.execute_input":"2022-03-04T11:19:24.154098Z","iopub.status.idle":"2022-03-04T11:19:25.710821Z","shell.execute_reply.started":"2022-03-04T11:19:24.154051Z","shell.execute_reply":"2022-03-04T11:19:25.709751Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from flask import Flask,jsonify,request,render_template\nimport joblib\napp=Flask(__name__)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:19:30.685768Z","iopub.execute_input":"2022-03-04T11:19:30.686055Z","iopub.status.idle":"2022-03-04T11:19:30.924841Z","shell.execute_reply.started":"2022-03-04T11:19:30.686015Z","shell.execute_reply":"2022-03-04T11:19:30.924285Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"@app.route(\"/\")\ndef hello_world():\n  return 'Hello World!'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@app.route('/index')\ndef index():\n  return Flask.render_template('index.html')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T08:48:09.337852Z","iopub.execute_input":"2022-03-04T08:48:09.338429Z","iopub.status.idle":"2022-03-04T08:48:09.343771Z","shell.execute_reply.started":"2022-03-04T08:48:09.338393Z","shell.execute_reply":"2022-03-04T08:48:09.342825Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"@app.route('/predict',methods=['POST'])\ndef predict():\n  clf=joblib.load('model.pkl')\n  pred=clf.predict()\n  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/LANL-Earthquake-Prediction/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32},error_bad_lines=False)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:19:34.998114Z","iopub.execute_input":"2022-03-04T11:19:34.998806Z","iopub.status.idle":"2022-03-04T11:22:35.088938Z","shell.execute_reply.started":"2022-03-04T11:19:34.998769Z","shell.execute_reply":"2022-03-04T11:22:35.088173Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#visualizing first 1000 datapoints\ntrain_ad_df = train['acoustic_data'].values[::1000]\ntrain_ttf_df = train['time_to_failure'].values[::1000]\n\n#function for plotting based on both features\ndef plot_ad_ttf_data(train_ad_df, train_ttf_df, title=\"Acoustic data and time to failure: first 1000 sampled data\"):\n    fig, ax1 = plt.subplots(figsize=(12, 8))\n    plt.title(title)\n    plt.plot(train_ad_df, color='r')\n    ax1.set_ylabel('acoustic data', color='r')\n    plt.legend(['acoustic data'], loc=(0.01, 0.95))\n    ax2 = ax1.twinx()\n    plt.plot(train_ttf_df, color='g')\n    ax2.set_ylabel('time to failure', color='b')\n    plt.legend(['time to failure'], loc=(0.01, 0.9))\n    plt.grid(True)\n\nplot_ad_ttf_data(train_ad_df, train_ttf_df)\ndel train_ad_df\ndel train_ttf_df","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:22:42.133114Z","iopub.execute_input":"2022-03-04T11:22:42.133422Z","iopub.status.idle":"2022-03-04T11:22:42.867940Z","shell.execute_reply.started":"2022-03-04T11:22:42.133391Z","shell.execute_reply":"2022-03-04T11:22:42.866861Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"In the above figure by plotting first 1000 datapoints we can clearly see that at the time of failure there is spike in acoustic data or seismic signal.","metadata":{}},{"cell_type":"code","source":"train.shape[0]\ntrain_ad_sample_df = train['acoustic_data'].values[:6291454]\ntrain_ttf_sample_df = train['time_to_failure'].values[:6291454]\nplot_ad_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure\")","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:25:24.601731Z","iopub.execute_input":"2022-03-04T11:25:24.602137Z","iopub.status.idle":"2022-03-04T11:25:26.966955Z","shell.execute_reply.started":"2022-03-04T11:25:24.602095Z","shell.execute_reply":"2022-03-04T11:25:26.966099Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def mad(x, axis=None):\n    \"\"\"\n    Mean Absolute Deviation\n    \"\"\"\n    \n    return np.mean(np.absolute(x - np.mean(x, axis)), axis)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:25:32.723804Z","iopub.execute_input":"2022-03-04T11:25:32.724570Z","iopub.status.idle":"2022-03-04T11:25:32.728510Z","shell.execute_reply.started":"2022-03-04T11:25:32.724529Z","shell.execute_reply":"2022-03-04T11:25:32.727817Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#https://www.kaggle.com/ilu000/1-private-lb-kernel-lanl-lgbm/\n#https://www.kaggle.com/tarunpaparaju/lanl-earthquake-prediction-signal-denoising/notebook\nSince data generated by seismogram is a mixture of artificial and actuals signals. Hence we need to denoise the acoustic data to get actual data. This actual underlying signal would be a better predictor of earthquake timing than the original raw signal, because it represents the actual seismic activity.\nSince acoustic data can be prone to noise, hence denoising the data can be a good move.  ","metadata":{}},{"cell_type":"code","source":"\ndef denoise_signal(x, wavelet='db4', level=1):\n    # Decompose to get the wavelet coefficients\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    \n    # Calculate sigma for threshold as defined in http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n\n    sigma = (1/0.6745) * mad(coeff[-level])\n\n    # Calculate the univeral threshold\n    u_thresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=u_thresh, mode='hard') for i in coeff[1:])\n    \n    # Reconstruct the signal using the thresholded coefficients\n    return pywt.waverec(coeff, wavelet, mode='per')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:25:41.636718Z","iopub.execute_input":"2022-03-04T11:25:41.637523Z","iopub.status.idle":"2022-03-04T11:25:41.643688Z","shell.execute_reply.started":"2022-03-04T11:25:41.637487Z","shell.execute_reply":"2022-03-04T11:25:41.642755Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_ad_df = train['acoustic_data'].values[::50000]\ntrain_ttf_df = train['time_to_failure'].values[::50000]\n\n#function for plotting based on both features\ndef plot_ad_ttf_data(x,y, title=\"Acoustic data and time to failure after denoising: first 50000 sampled data\"):\n    fig, ax1 = plt.subplots(figsize=(12, 8))\n    plt.title(title)\n    plt.plot(denoise_signal(x), color='r')\n    ax1.set_ylabel('acoustic data', color='r')\n    plt.legend(['acoustic data'], loc=(0.01, 0.95))\n    ax2 = ax1.twinx()\n    plt.plot(y, color='g')\n    ax2.set_ylabel('time to failure', color='b')\n    plt.legend(['time to failure'], loc=(0.01, 0.9))\n    plt.grid(True)\n\nplot_ad_ttf_data(train_ad_df, train_ttf_df)\ndel train_ad_df\ndel train_ttf_df","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:25:45.740555Z","iopub.execute_input":"2022-03-04T11:25:45.740814Z","iopub.status.idle":"2022-03-04T11:25:46.408259Z","shell.execute_reply.started":"2022-03-04T11:25:45.740786Z","shell.execute_reply":"2022-03-04T11:25:46.407663Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/LANL-Earthquake-Prediction/test/seg_fb8af5.csv')\ntest_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:25:52.212414Z","iopub.execute_input":"2022-03-04T11:25:52.212854Z","iopub.status.idle":"2022-03-04T11:25:52.239595Z","shell.execute_reply.started":"2022-03-04T11:25:52.212815Z","shell.execute_reply":"2022-03-04T11:25:52.238805Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Engineering:** \nSo since our data we just have two features. So we will try to add some more statistical features that might have worked for researchers in past while working with this type of data.Lets create a function to generate some statistical features based on the training data\n\nAlso as we can see that our test data has 150000 rows ,hnece we are going to split our train too in chunks/segments of 150000.","metadata":{}},{"cell_type":"code","source":"seg_size = 150000\nsegments = int(np.floor(train.shape[0] / seg_size))\nSAMPLE_RATE = 4000\nsegments","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:25:57.812508Z","iopub.execute_input":"2022-03-04T11:25:57.812778Z","iopub.status.idle":"2022-03-04T11:25:57.819089Z","shell.execute_reply.started":"2022-03-04T11:25:57.812750Z","shell.execute_reply":"2022-03-04T11:25:57.818247Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def create_features(segment_id, segment, X, targets=True):\n    xc = pd.Series(segment['acoustic_data'].values)\n    zc = np.fft.fft(xc)\n    realFFT = np.real(zc)\n    imagFFT = np.imag(zc)\n    \n    X.loc[seg_id, 'mean'] = xc.mean()\n    X.loc[seg_id, 'std'] = xc.std()\n    X.loc[seg_id, 'max'] = xc.max()\n    X.loc[seg_id, 'min'] = xc.min()\n    X.loc[seg_id, 'sum'] = xc.sum()\n    X.loc[seg_id, 'mad'] = xc.mad()\n    X.loc[seg_id, 'kurt'] = xc.kurtosis()\n    X.loc[seg_id, 'skew'] = xc.skew()\n    X.loc[seg_id, 'med'] = xc.median()\n    \n    X.loc[seg_id, 'q95'] = np.quantile(xc, 0.95)\n    X.loc[seg_id, 'q99'] = np.quantile(xc, 0.99)\n    X.loc[seg_id, 'q05'] = np.quantile(xc, 0.05)\n    X.loc[seg_id, 'q01'] = np.quantile(xc, 0.01)\n    \n    X.loc[seg_id, 'abs_mean'] = np.abs(xc).mean()\n    X.loc[seg_id, 'abs_max'] = np.abs(xc).max()\n    X.loc[seg_id, 'abs_min'] = np.abs(xc).min()\n    X.loc[seg_id, 'abs_q95'] = np.quantile(np.abs(xc), 0.95)\n    X.loc[seg_id, 'abs_q99'] = np.quantile(np.abs(xc), 0.99)\n    X.loc[seg_id, 'abs_q05'] = np.quantile(np.abs(xc), 0.05)\n    X.loc[seg_id, 'abs_q01'] = np.quantile(np.abs(xc), 0.01)\n    \n    X.loc[seg_id, 'Rmean'] = realFFT.mean()\n    X.loc[seg_id, 'Rstd'] = realFFT.std()\n    X.loc[seg_id, 'Rmax'] = realFFT.max()\n    X.loc[seg_id, 'Rmin'] = realFFT.min()\n    \n    X.loc[seg_id, 'Imean'] = imagFFT.mean()\n    X.loc[seg_id, 'Istd'] = imagFFT.std()\n    X.loc[seg_id, 'Imax'] = imagFFT.max()\n    X.loc[seg_id, 'Imin'] = imagFFT.min()\n    \n    X.loc[seg_id, 'std_first_50000'] = xc[:50000].std()\n    X.loc[seg_id, 'std_last_50000'] = xc[-50000:].std()\n    X.loc[seg_id, 'std_first_25000'] = xc[:25000].std()\n    X.loc[seg_id, 'std_last_25000'] = xc[-25000:].std()\n    X.loc[seg_id, 'std_first_10000'] = xc[:10000].std()\n    X.loc[seg_id, 'std_last_10000'] = xc[-10000:].std()\n    \n    X.loc[seg_id, 'Rstd_first_50000'] = realFFT[:50000].std()\n    X.loc[seg_id, 'Rstd_last_50000'] = realFFT[-50000:].std()\n    X.loc[seg_id, 'Rstd_first_25000'] = realFFT[:25000].std()\n    X.loc[seg_id, 'Rstd_last_25000'] = realFFT[-25000:].std()\n    X.loc[seg_id, 'Rstd_first_10000'] = realFFT[:10000].std()\n    X.loc[seg_id, 'Rstd_last_10000'] = realFFT[-10000:].std()\n    \n    X.loc[seg_id, 'mean_first_50000'] = xc[:50000].mean()\n    X.loc[seg_id, 'mean_last_50000'] = xc[-50000:].mean()\n    X.loc[seg_id, 'mean_first_25000'] = xc[:25000].mean()\n    X.loc[seg_id, 'mean_last_25000'] = xc[-25000:].mean()\n    X.loc[seg_id, 'mean_first_10000'] = xc[:10000].mean()\n    X.loc[seg_id, 'mean_last_10000'] = xc[-10000:].mean()\n    \n    X.loc[seg_id, 'Rmean_first_50000'] = realFFT[:50000].mean()\n    X.loc[seg_id, 'Rmean_last_50000'] = realFFT[-50000:].mean()\n    X.loc[seg_id, 'Rmean_first_25000'] = realFFT[:25000].mean()\n    X.loc[seg_id, 'Rmean_last_25000'] = realFFT[-25000:].mean()\n    X.loc[seg_id, 'Rmean_first_10000'] = realFFT[:10000].mean()\n    X.loc[seg_id, 'Rmean_last_10000'] = realFFT[-10000:].mean()\n    \n    X.loc[seg_id, 'max_first_50000'] = xc[:50000].max()\n    X.loc[seg_id, 'max_last_50000'] = xc[-50000:].max()\n    X.loc[seg_id, 'max_first_25000'] = xc[:25000].max()\n    X.loc[seg_id, 'max_last_25000'] = xc[-25000:].max()\n    X.loc[seg_id, 'max_first_10000'] = xc[:10000].max()\n    X.loc[seg_id, 'max_last_10000'] = xc[-10000:].max()\n    \n    X.loc[seg_id, 'Rmax_first_50000'] = realFFT[:50000].max()\n    X.loc[seg_id, 'Rmax_last_50000'] = realFFT[-50000:].max()\n    X.loc[seg_id, 'Rmax_first_25000'] = realFFT[:25000].max()\n    X.loc[seg_id, 'Rmax_last_25000'] = realFFT[-25000:].max()\n    X.loc[seg_id, 'Rmax_first_10000'] = realFFT[:10000].max()\n    X.loc[seg_id, 'Rmax_last_10000'] = realFFT[-10000:].max()\n    \n    X.loc[seg_id, 'min_first_50000'] = xc[:50000].min()\n    X.loc[seg_id, 'min_last_50000'] = xc[-50000:].min()\n    X.loc[seg_id, 'min_first_25000'] = xc[:25000].min()\n    X.loc[seg_id, 'min_last_25000'] = xc[-25000:].min()\n    X.loc[seg_id, 'min_first_10000'] = xc[:10000].min()\n    X.loc[seg_id, 'min_last_10000'] = xc[-10000:].min()\n    \n    X.loc[seg_id, 'Rmin_first_50000'] = realFFT[:50000].min()\n    X.loc[seg_id, 'Rmin_last_50000'] = realFFT[-50000:].min()\n    X.loc[seg_id, 'Rmin_first_25000'] = realFFT[:25000].min()\n    X.loc[seg_id, 'Rmin_last_25000'] = realFFT[-25000:].min()\n    X.loc[seg_id, 'Rmin_first_10000'] = realFFT[:10000].min()\n    X.loc[seg_id, 'Rmin_last_10000'] = realFFT[-10000:].min()\n    \n    #To get Rolling features we are refrencing from kernel: https://www.kaggle.com/wimwim/rolling-quantiles\n    for windows in [5, 10, 50, 100, 500, 1000, 5000, 10000]:\n        x_roll_std = xc.rolling(windows).std().dropna().values\n        x_roll_mean = xc.rolling(windows).mean().dropna().values\n        X.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n        X.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n        X.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n        X.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n        X.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n        X.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n        X.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n        X.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n        X.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n        X.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n        X.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n        \n        X.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n        X.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n        X.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n        X.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n        X.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n        X.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n        X.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n        X.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n    if targets:\n        X.loc[seg_id, 'time_to_failure'] = segment['time_to_failure'].values[-1]","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:47:08.072715Z","iopub.execute_input":"2022-03-04T11:47:08.073195Z","iopub.status.idle":"2022-03-04T11:47:08.110797Z","shell.execute_reply.started":"2022-03-04T11:47:08.073150Z","shell.execute_reply":"2022-03-04T11:47:08.110198Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\n#Calculate features and add to dataframe train_df \n\ntrain_df = pd.DataFrame(index=range(segments), dtype=np.float64)\n\nfor seg_id in tqdm_notebook(range(segments)):\n    segment = train.iloc[seg_id*seg_size:seg_id*seg_size+seg_size]\n    create_features(seg_id, segment, train_df)\n   ","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:28:48.823096Z","iopub.execute_input":"2022-03-04T11:28:48.823634Z","iopub.status.idle":"2022-03-04T11:47:08.070133Z","shell.execute_reply.started":"2022-03-04T11:28:48.823600Z","shell.execute_reply":"2022-03-04T11:47:08.069080Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:49:12.003162Z","iopub.execute_input":"2022-03-04T11:49:12.004154Z","iopub.status.idle":"2022-03-04T11:49:12.046602Z","shell.execute_reply.started":"2022-03-04T11:49:12.004105Z","shell.execute_reply":"2022-03-04T11:49:12.045768Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"X_tr=train_df.drop(['time_to_failure'],axis=1)\ny_tr = train_df[['time_to_failure']].copy()\ny_tr","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:49:18.477994Z","iopub.execute_input":"2022-03-04T11:49:18.478298Z","iopub.status.idle":"2022-03-04T11:49:18.507538Z","shell.execute_reply.started":"2022-03-04T11:49:18.478255Z","shell.execute_reply":"2022-03-04T11:49:18.506752Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(26, 24))\nplt.title(\"\")\ncols = list(np.abs(X_tr.corrwith(y_tr['time_to_failure'])).sort_values(ascending=False).head(24).index)\nfor i, col in enumerate(cols):\n    ax1=plt.subplot(6,4, i+1)\n    plt.plot(X_tr[col], color='blue')\n    plt.title(col)\n    ax1.set_ylabel(col, color='b')\n    ax2 = ax1.twinx()\n    plt.plot(y_tr, color='g')\n    ax2.set_ylabel('time_to_failure', color='g')\n    plt.legend([col, 'time_to_failure'], loc=(0.875, 0.9))\n    plt.grid(False)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:49:22.986905Z","iopub.execute_input":"2022-03-04T11:49:22.987692Z","iopub.status.idle":"2022-03-04T11:49:29.360708Z","shell.execute_reply.started":"2022-03-04T11:49:22.987637Z","shell.execute_reply":"2022-03-04T11:49:29.360018Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Since our test data is present in lots of segments hence we are going to initialize them in test_df dataframe.","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/LANL-Earthquake-Prediction/sample_submission.csv', index_col='seg_id')\ntest_df = pd.DataFrame(columns=train_df.columns, dtype=np.float64, index=submission.index)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:54:20.527686Z","iopub.execute_input":"2022-03-04T11:54:20.528073Z","iopub.status.idle":"2022-03-04T11:54:20.558963Z","shell.execute_reply.started":"2022-03-04T11:54:20.528036Z","shell.execute_reply":"2022-03-04T11:54:20.558277Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Fearture Selection:\nAs \nThere are two approaches that can be used for feature selection:\nApproach 1: Correlation\nSince we have large number of features, we will use the correlation matrix to select the features that have a high correlation for time_to_failure. ","metadata":{}},{"cell_type":"code","source":"corrMat = train_df.corr()\ncorrTarget = abs(corrMat['time_to_failure'])\ncorrTarget = corrTarget[corrTarget > 0.5]\nhigh_corr_features = corrTarget.index.drop(['time_to_failure'])\nhigh_corr_features","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:54:23.937634Z","iopub.execute_input":"2022-03-04T11:54:23.938470Z","iopub.status.idle":"2022-03-04T11:54:24.547891Z","shell.execute_reply.started":"2022-03-04T11:54:23.938427Z","shell.execute_reply":"2022-03-04T11:54:24.546866Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"X_train = train_df[high_corr_features]\ny_train = train_df['time_to_failure']\nX_train","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:54:30.808172Z","iopub.execute_input":"2022-03-04T11:54:30.808634Z","iopub.status.idle":"2022-03-04T11:54:30.833612Z","shell.execute_reply.started":"2022-03-04T11:54:30.808600Z","shell.execute_reply":"2022-03-04T11:54:30.832694Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"test_path = '/kaggle/input/LANL-Earthquake-Prediction/test'\nfor seg_id in tqdm_notebook(test_df.index):\n    seg_file = seg_id + '.csv'\n    seg = pd.read_csv(os.path.join(test_path, seg_file))\n    create_features(seg_id, seg, test_df, targets=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:54:34.618510Z","iopub.execute_input":"2022-03-04T11:54:34.618808Z","iopub.status.idle":"2022-03-04T12:05:23.645253Z","shell.execute_reply.started":"2022-03-04T11:54:34.618775Z","shell.execute_reply":"2022-03-04T12:05:23.644276Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"final_X_test = test_df[high_corr_features]","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:05:38.963859Z","iopub.execute_input":"2022-03-04T12:05:38.964161Z","iopub.status.idle":"2022-03-04T12:05:38.969911Z","shell.execute_reply.started":"2022-03-04T12:05:38.964128Z","shell.execute_reply":"2022-03-04T12:05:38.968733Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"\n# The function below scatters the data points by each feature and the corresponding time_to_failure ","metadata":{}},{"cell_type":"code","source":"def plot_feature_scatter(features, X=train_df):\n    i = 0\n    plt.figure()\n    nlines = int(len(features)/2) \n    fig, ax = plt.subplots(nlines, 2, figsize=(20, 5*nlines ))\n    for feature in features:\n        i+=1\n        plt.subplot(nlines,2,i)\n        ax[int((i-1)/2)][(i+1) % 2].set_xlabel(feature)\n        ax[int((i-1)/2)][(i+1) % 2].set_ylabel('time_to_failure')\n        plt.title('{} - time_to_falure correlation)'.format(feature), color='r')\n        plt.scatter(x = X[feature], y = X['time_to_failure'])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:05:43.082085Z","iopub.execute_input":"2022-03-04T12:05:43.082809Z","iopub.status.idle":"2022-03-04T12:05:43.091184Z","shell.execute_reply.started":"2022-03-04T12:05:43.082761Z","shell.execute_reply":"2022-03-04T12:05:43.090184Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"plot_feature_scatter(high_corr_features)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:05:47.356018Z","iopub.execute_input":"2022-03-04T12:05:47.356406Z","iopub.status.idle":"2022-03-04T12:05:50.359092Z","shell.execute_reply.started":"2022-03-04T12:05:47.356367Z","shell.execute_reply":"2022-03-04T12:05:50.358406Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"The code below uses the histplot function to describe the histogram and density of the features, red for train data and blue for test data.","metadata":{}},{"cell_type":"code","source":"def plot_histplot_features(features, colors=['red', 'blue'], df1=X_train):\n    i = 0\n    plt.figure()\n    nlines = int(len(features)/2)\n    fig, ax = plt.subplots(nlines,2,figsize=(20,5*nlines))\n    for feature in features:\n        i += 1\n        plt.subplot(nlines,2,i)\n        sns.histplot(df1[feature],color=colors[0], kde=True, bins=40, label='train')\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:07:12.053772Z","iopub.execute_input":"2022-03-04T12:07:12.054350Z","iopub.status.idle":"2022-03-04T12:07:12.061449Z","shell.execute_reply.started":"2022-03-04T12:07:12.054309Z","shell.execute_reply":"2022-03-04T12:07:12.060284Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"plot_histplot_features(high_corr_features)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:07:15.586756Z","iopub.execute_input":"2022-03-04T12:07:15.587519Z","iopub.status.idle":"2022-03-04T12:07:19.890286Z","shell.execute_reply.started":"2022-03-04T12:07:15.587470Z","shell.execute_reply":"2022-03-04T12:07:19.889462Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Now we will scale the features to better fit the normal distribution and will try to visualize our data again. We will scale with both train and test data. Yellow corresponds to train data and Red corresponds to test data. ","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(pd.concat([X_train, final_X_test]))\nscaled_X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\nscaled_X_test = pd.DataFrame(scaler.transform(final_X_test), columns=final_X_test.columns, index=final_X_test.index)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:07:39.580626Z","iopub.execute_input":"2022-03-04T12:07:39.581307Z","iopub.status.idle":"2022-03-04T12:07:39.598704Z","shell.execute_reply.started":"2022-03-04T12:07:39.581258Z","shell.execute_reply":"2022-03-04T12:07:39.597720Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"plot_histplot_features(high_corr_features, colors='yellow', df1=scaled_X_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:07:42.773801Z","iopub.execute_input":"2022-03-04T12:07:42.774907Z","iopub.status.idle":"2022-03-04T12:07:46.724342Z","shell.execute_reply.started":"2022-03-04T12:07:42.774851Z","shell.execute_reply":"2022-03-04T12:07:46.723535Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# **After scaling our features we will plot the highly correlated features with time of failure.**","metadata":{}},{"cell_type":"code","source":"def plot_feature_ttf(features):\n    i = 0\n    plt.figure()\n    nlines = int(len(features)/2)\n    fig, ax = plt.subplots(nlines,2,figsize=(32,8*nlines))\n    for feature in features:\n        i += 1\n        plt.subplot(nlines,2,i)\n        plt.title('({}) and time to failure'.format(feature))\n        plt.plot(X_train[feature], color='r')\n        ax[int((i-1)/2)][(i+1) % 2].set_xlabel('training samples')\n        ax[int((i-1)/2)][(i+1) % 2].set_ylabel('acoustic data ({})'.format(feature), color='r')\n        plt.legend(['acoustic data ({})'.format(feature)], loc=(0.01, 0.95))\n        ax2 = ax[int((i-1)/2)][(i+1) % 2].twinx()\n        plt.plot(y_train, color='b')\n        ax2.set_ylabel('time to failure', color='b')\n        plt.legend(['time to failure'], loc=(0.01, 0.9))\n        plt.grid(True)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:11:24.760449Z","iopub.execute_input":"2022-03-04T12:11:24.760787Z","iopub.status.idle":"2022-03-04T12:11:24.769597Z","shell.execute_reply.started":"2022-03-04T12:11:24.760749Z","shell.execute_reply":"2022-03-04T12:11:24.768852Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"plot_feature_ttf(high_corr_features)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:11:28.543294Z","iopub.execute_input":"2022-03-04T12:11:28.544322Z","iopub.status.idle":"2022-03-04T12:11:33.748153Z","shell.execute_reply.started":"2022-03-04T12:11:28.544275Z","shell.execute_reply":"2022-03-04T12:11:33.747219Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Observation: \nWe can clearly see that during the time of failure there is spike in the values of the statistically generated features.","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(scaled_X_train, y_tr, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:11:39.621995Z","iopub.execute_input":"2022-03-04T12:11:39.624585Z","iopub.status.idle":"2022-03-04T12:11:39.638979Z","shell.execute_reply.started":"2022-03-04T12:11:39.624541Z","shell.execute_reply":"2022-03-04T12:11:39.637659Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# SVM+RBF Kernel model","metadata":{}},{"cell_type":"markdown","source":"We are using gridsearch CV for Hyperparameter tuning and find the best C and epsilon value.","metadata":{}},{"cell_type":"code","source":"parameters = {'kernel': ('linear', 'rbf','poly'), 'C':[0.01, 0.1, 1, 10, 100],'epsilon':[0.1,0.2,0.3,0.4,0.5,0.6]}\nsvr = SVR(gamma = 'scale')\nclf = GridSearchCV(svr, parameters)\nclf.fit(X_train,y_train)\nclf.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:11:42.620791Z","iopub.execute_input":"2022-03-04T12:11:42.621347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SVR(kernel='rbf', C=1, epsilon=0.6, gamma='scale')\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T21:59:09.938441Z","iopub.execute_input":"2022-03-03T21:59:09.938776Z","iopub.status.idle":"2022-03-03T21:59:10.386804Z","shell.execute_reply.started":"2022-03-03T21:59:09.938738Z","shell.execute_reply":"2022-03-03T21:59:10.385909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_valid)\nmae = mean_absolute_error(y_pred, y_valid)\nprint('mae = {}'.format(mae))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T21:59:13.9725Z","iopub.execute_input":"2022-03-03T21:59:13.972767Z","iopub.status.idle":"2022-03-03T21:59:14.085349Z","shell.execute_reply.started":"2022-03-03T21:59:13.972739Z","shell.execute_reply":"2022-03-03T21:59:14.084591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CatBoost algorithm","metadata":{}},{"cell_type":"code","source":"train_pool = Pool(X_train, y_train)\nprediction = np.zeros(len(X_valid))\nm = CatBoostRegressor(iterations=10000, loss_function='MAE', boosting_type='Ordered')\nm.fit(X_train, y_train, silent=True)\nm.best_score_","metadata":{"execution":{"iopub.status.busy":"2022-03-03T23:08:55.756789Z","iopub.execute_input":"2022-03-03T23:08:55.757128Z","iopub.status.idle":"2022-03-03T23:10:50.616722Z","shell.execute_reply.started":"2022-03-03T23:08:55.757093Z","shell.execute_reply":"2022-03-03T23:10:50.615857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_file = \"model.pickle\"\nwith open(model_file,'wb') as f:\n    pickle.dump(m, f)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:13:36.148565Z","iopub.execute_input":"2022-03-04T09:13:36.148949Z","iopub.status.idle":"2022-03-04T09:13:36.246537Z","shell.execute_reply.started":"2022-03-04T09:13:36.148850Z","shell.execute_reply":"2022-03-04T09:13:36.245026Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"joblib.dump(m,'model.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:13:42.654433Z","iopub.execute_input":"2022-03-04T09:13:42.654745Z","iopub.status.idle":"2022-03-04T09:13:42.671685Z","shell.execute_reply.started":"2022-03-04T09:13:42.654696Z","shell.execute_reply":"2022-03-04T09:13:42.670764Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# XgBoost Algorithm","metadata":{}},{"cell_type":"code","source":"xgb_model = xgb.XGBRegressor()\nxgb_model.fit(X_train,y_train.values)\npred = xgb_model.predict(X_valid)#prediction without hyperparameter tuning\n","metadata":{"execution":{"iopub.status.busy":"2022-03-03T22:13:00.104983Z","iopub.execute_input":"2022-03-03T22:13:00.10527Z","iopub.status.idle":"2022-03-03T22:13:00.902314Z","shell.execute_reply.started":"2022-03-03T22:13:00.10524Z","shell.execute_reply":"2022-03-03T22:13:00.901539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyperparameter tuning with XGBoost \n# creating a KFold object with 3 splits\n\nfolds = KFold(n_splits= 3, shuffle= True, random_state= 101)\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.01, 0.1, 0.2, 0.3], \n             'subsample': [0.3, 0.6, 0.9, 1],\n              'n_estimators' : [5, 10, 15, 20],\n              'max_depth' :[2,4,6,8]          \n             }          \n\n\n# specify model\nxgb_model = xgb.XGBRegressor()\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model, \n                        param_grid = param_grid, \n                        scoring='neg_mean_absolute_error', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True, \n                        n_jobs= -1)      ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T22:13:09.283237Z","iopub.execute_input":"2022-03-03T22:13:09.284337Z","iopub.status.idle":"2022-03-03T22:13:09.289911Z","shell.execute_reply.started":"2022-03-03T22:13:09.284255Z","shell.execute_reply":"2022-03-03T22:13:09.289312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cv.fit(X_train,y_train.values)   #train the model","metadata":{"execution":{"iopub.status.busy":"2022-03-03T22:13:13.796953Z","iopub.execute_input":"2022-03-03T22:13:13.797788Z","iopub.status.idle":"2022-03-03T22:54:58.233046Z","shell.execute_reply.started":"2022-03-03T22:13:13.797738Z","shell.execute_reply":"2022-03-03T22:54:58.232031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(' neg_mean_absolute_error:',model_cv.best_score_,'using',model_cv.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:52:38.187837Z","iopub.execute_input":"2022-03-03T14:52:38.188625Z","iopub.status.idle":"2022-03-03T14:52:38.194058Z","shell.execute_reply.started":"2022-03-03T14:52:38.188575Z","shell.execute_reply":"2022-03-03T14:52:38.193269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define the model with the best parameter and training oif the model to make the predictions\nxgb_model = xgb.XGBRegressor(learning_rate= 0.2, max_depth= 2, n_estimators= 15, subsample= 0.9)\n\nxgb_model.fit(X_train,y_train.values)\n\npred = xgb_model.predict(X_valid)\n#pred","metadata":{"execution":{"iopub.status.busy":"2022-03-03T23:04:45.172773Z","iopub.execute_input":"2022-03-03T23:04:45.173123Z","iopub.status.idle":"2022-03-03T23:04:45.265202Z","shell.execute_reply.started":"2022-03-03T23:04:45.173085Z","shell.execute_reply":"2022-03-03T23:04:45.2643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mytable=PrettyTable([\"Model\",\"Best hyperparameter\",\"MAE\"])\nmytable.add_row([\"SVR+RBF\",\"C=1,epsilon=0.6\",\"2.091950549780848\"])\nmytable.add_row([\"CatBoost Regressor\",\"iteration=10000\",\"1.0722845986656637\"])\nmytable.add_row([\"XBoost\",\"Kfold=5\",\"2.0792648604931623\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-03T16:17:03.984106Z","iopub.execute_input":"2022-03-03T16:17:03.984474Z","iopub.status.idle":"2022-03-03T16:17:03.993888Z","shell.execute_reply.started":"2022-03-03T16:17:03.984438Z","shell.execute_reply":"2022-03-03T16:17:03.992974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we are getting best mean absolute error for catboost, Hence we will be using the same for predictions.","metadata":{}},{"cell_type":"code","source":"prediction=m.predict(final_X_test)\nsubmission = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')\nsubmission['time_to_failure'] = prediction\nsubmission.to_csv('submission_cat.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T23:30:34.596505Z","iopub.execute_input":"2022-03-03T23:30:34.597375Z","iopub.status.idle":"2022-03-03T23:30:34.646573Z","shell.execute_reply.started":"2022-03-03T23:30:34.597326Z","shell.execute_reply":"2022-03-03T23:30:34.645713Z"},"trusted":true},"execution_count":null,"outputs":[]}]}